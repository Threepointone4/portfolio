[
  {
    "objectID": "posts/Building_components_of_Stable_Diffusion/index.html",
    "href": "posts/Building_components_of_Stable_Diffusion/index.html",
    "title": "Building components of Stable Diffusion",
    "section": "",
    "text": "In this we will try to build the Stable Diffusion pipeline we saw in previous blog.\nJust a recap ,\nWhat are the blocks till now we saw?\n\n1. Model which extracts noise and outputs less noise image.\nFor this we generally use specific type of models called Unet.\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\").to(DEVICE)\n\n\n2. Model which embeds the text into embedding.\nFor this we use model called CLIP. As we are dealing with text, we need to load tokenizer as well along with the model.\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(DEVICE)\n\n\n3. Model which compresses and de-compressed images. ( output of these compression models are called as latents).\nThe model which are used for this is called as VAE. Image compression is used during training. During inference as we are starting from noise, we will not be using VAE.\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(DEVICE)\nLets understand what happens inside StableDiffusionPipeline.\nOur input will be text (prompt).\nprompt = [\"man riding blue bike with red dress\"]\nNow we will tokenize and convert it into embedding using CLIP tokenizer and CLIP encoder respectively.\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ntext_embeddings = text_encoder(text_input.input_ids.to(DEVICE))[0]\n\nNow we need noise also as input. (input latent).\nWe will use random weight with proper input shape which is required for Unet model which we have loaded.\nlatents = torch.randn((batch_size, unet.in_channels, height // 8, width // 8))\nlatents = latents.to(DEVICE)\nNow we have both the inputs, we can give it to Unet to generate output latent. One issue with this is, generally model tends to overfit to the text given. This leads to less creative and more aligned with text outputs.\nSo what we do is, generate embedding for empty string and concat with text embedding. The intiution is, we will generate image completly based on input , one more completely creative. We will average both with some condition to get best of both.\nfinal_prediction = creative_predection + guidance_scale * predection_based_on_text\nThis guidance_scale param lets user to control. The smaller value tends to generate more creative and less accurate and the higher value generated accurate but less creative. According to documentation 7.5 is the best between both.\nLets code this\n#We will remove noise by X times\nfor i, t in enumerate(tqdm(scheduler.timesteps)):\n    # As we generate for both creative and text , we will have 2 noise inputs\n    input = torch.cat([latents] * 2)\n    input = scheduler.scale_model_input(input, t)\n\n    # Given to unet for prediction\n    with torch.no_grad(): \n        pred = unet(input, t, encoder_hidden_states=text_embeddings).sample\n\n    # perform guidance\n    pred_uncond, pred_text = pred.chunk(2)\n    pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n\n    # compute the \"previous\" noisy sample\n    latents = scheduler.step(pred, t, latents).prev_sample\nNow the last step. We will convert over latent into proper image using VAE.\nwith torch.no_grad():\n    image = vae.decode(1 / 0.18215 * latents).sample\n\n\nIn the next post we will be understanding and building negetive prompt in stable diffusion."
  },
  {
    "objectID": "posts/Introduction_to_Stable_Diffusion/index.html",
    "href": "posts/Introduction_to_Stable_Diffusion/index.html",
    "title": "Introduction to Stable Diffusion",
    "section": "",
    "text": "Image Credit link\n\n\nIn this blog we will go through how stable diffusion models work.This Blog is mainly Summary of the Fastai video. We will be using diffusers library by huggingface.\nLets understand the intuition of these.\nLets say we have alot of images. We will add random gaussian noise to the images. Now we have images with somewhat noise. Lets take these images and train a model which extracts the noise we have added.\nNow we have model which given noisy image extract noise and we can have image generated.\n\nAs our model knows to extract somewhat noise , so given so pure noise we can iterate for X steps to the get some tangable output.\n\n\n\nImage Credit link\n\n\nwhy not run it only once?\n\n\n\nOutput for single noise extraction\n\n\nAs you can see from above example.In first iteration it may extract some noise. Again it will be given to extract. This when we do for X steps we get the final output.\nNow instead of random image generated, can we guide the model to generate specfic images? Yes.\nIn the above model, along with noisy image if we also give embedding of text as input to train. The model will understand what to generate. This method is called as Classifier-Free Guidance.\nLets see quick example using huggingface pipeline\n\npipe2 = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16).to(\"cuda\")\n\nprompt = \"a photograph of an astronaut riding a horse\"\n\npipe2(prompt).images[0]\n\nGenerally images are 3d with alot of values. A small 512*512 image will have 786432 values. But from information persepective alot of values donâ€™t add value. So we use VAE. This model compress image into small dimension, which gives only important values which actually adds value. This will reduce our inputs which will reduce the computation.\nWe may need to use de compression during output to get proper image.\nWhat are the blocks till now we saw?\n\nModel which extracts noise and outputs less noise image.\nModel which embeds the text into embedding.\nModel which compresses and de-compressed images. ( output of these compression models are called as latents)\n\n\nIn next blog we will go through components of Stable Diffusion with code. Next>>"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vishwas Pai",
    "section": "",
    "text": "Building components of Stable Diffusion\n\n\n\n\n\n\n\nCV\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2022\n\n\nVishwas Pai\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Stable Diffusion\n\n\n\n\n\n\n\nCV\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2022\n\n\nVishwas Pai\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Socials"
  }
]