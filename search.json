[
  {
    "objectID": "posts/Building_components_of_Stable_Diffusion/index.html",
    "href": "posts/Building_components_of_Stable_Diffusion/index.html",
    "title": "Building components of Stable Diffusion",
    "section": "",
    "text": "In this we will try to build the Stable Diffusion pipeline we saw in previous blog.\nJust a recap ,\nWhat are the blocks till now we saw?\n\n1. Model which extracts noise and outputs less noise image.\nFor this we generally use specific type of models called Unet.\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\").to(DEVICE)\n\n\n2. Model which embeds the text into embedding.\nFor this we use model called CLIP. As we are dealing with text, we need to load tokenizer as well along with the model.\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(DEVICE)\n\n\n3. Model which compresses and de-compressed images. ( output of these compression models are called as latents).\nThe model which are used for this is called as VAE. Image compression is used during training. During inference as we are starting from noise, we will not be using VAE.\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(DEVICE)\nLets understand what happens inside StableDiffusionPipeline.\nOur input will be text (prompt).\nprompt = [\"man riding blue bike with red dress\"]\nNow we will tokenize and convert it into embedding using CLIP tokenizer and CLIP encoder respectively.\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ntext_embeddings = text_encoder(text_input.input_ids.to(DEVICE))[0]\n\nNow we need noise also as input. (input latent).\nWe will use random weight with proper input shape which is required for Unet model which we have loaded.\nlatents = torch.randn((batch_size, unet.in_channels, height // 8, width // 8))\nlatents = latents.to(DEVICE)\nNow we have both the inputs, we can give it to Unet to generate output latent. One issue with this is, generally model tends to overfit to the text given. This leads to less creative and more aligned with text outputs.\nSo what we do is, generate embedding for empty string and concat with text embedding. The intiution is, we will generate image completly based on input , one more completely creative. We will average both with some condition to get best of both.\nfinal_prediction = creative_predection + guidance_scale * predection_based_on_text\nThis guidance_scale param lets user to control. The smaller value tends to generate more creative and less accurate and the higher value generated accurate but less creative. According to documentation 7.5 is the best between both.\nLets code this\n#We will remove noise by X times\nfor i, t in enumerate(tqdm(scheduler.timesteps)):\n    # As we generate for both creative and text , we will have 2 noise inputs\n    input = torch.cat([latents] * 2)\n    input = scheduler.scale_model_input(input, t)\n\n    # Given to unet for prediction\n    with torch.no_grad(): \n        pred = unet(input, t, encoder_hidden_states=text_embeddings).sample\n\n    # perform guidance\n    pred_uncond, pred_text = pred.chunk(2)\n    pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n\n    # compute the \"previous\" noisy sample\n    latents = scheduler.step(pred, t, latents).prev_sample\nNow the last step. We will convert over latent into proper image using VAE.\nwith torch.no_grad():\n    image = vae.decode(1 / 0.18215 * latents).sample\n\n\nIn the next post we will be understanding and building negetive prompt in stable diffusion."
  },
  {
    "objectID": "posts/Introduction_to_Stable_Diffusion/index.html",
    "href": "posts/Introduction_to_Stable_Diffusion/index.html",
    "title": "Introduction to Stable Diffusion",
    "section": "",
    "text": "Image Credit link\n\n\nIn this blog we will go through how stable diffusion models work.This Blog is mainly Summary of the Fastai video. We will be using diffusers library by huggingface.\nLets understand the intuition of these.\nLets say we have alot of images. We will add random gaussian noise to the images. Now we have images with somewhat noise. Lets take these images and train a model which extracts the noise we have added.\nNow we have model which given noisy image extract noise and we can have image generated.\n\nAs our model knows to extract somewhat noise , so given so pure noise we can iterate for X steps to the get some tangable output.\n\n\n\nImage Credit link\n\n\nwhy not run it only once?\n\n\n\nOutput for single noise extraction\n\n\nAs you can see from above example.In first iteration it may extract some noise. Again it will be given to extract. This when we do for X steps we get the final output.\nNow instead of random image generated, can we guide the model to generate specfic images? Yes.\nIn the above model, along with noisy image if we also give embedding of text as input to train. The model will understand what to generate. This method is called as Classifier-Free Guidance.\nLets see quick example using huggingface pipeline\n\npipe2 = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16).to(\"cuda\")\n\nprompt = \"a photograph of an astronaut riding a horse\"\n\npipe2(prompt).images[0]\n\nGenerally images are 3d with alot of values. A small 512*512 image will have 786432 values. But from information persepective alot of values don’t add value. So we use VAE. This model compress image into small dimension, which gives only important values which actually adds value. This will reduce our inputs which will reduce the computation.\nWe may need to use de compression during output to get proper image.\nWhat are the blocks till now we saw?\n\nModel which extracts noise and outputs less noise image.\nModel which embeds the text into embedding.\nModel which compresses and de-compressed images. ( output of these compression models are called as latents)\n\n\nIn next blog we will go through components of Stable Diffusion with code. Next>>"
  },
  {
    "objectID": "posts/Negetive_Prompts_in_Stable_Diffusion/index.html",
    "href": "posts/Negetive_Prompts_in_Stable_Diffusion/index.html",
    "title": "Negative Prompts in Stable Diffusion",
    "section": "",
    "text": "In this post we will see how negative prompt work in stable diffusion.\nIntuition is lets say input_text = \"man in blue dress\" and this generates a output latent p1. Now one more input called input_text2 = \"blue\" and this generates a output latent p2. Now at a high level p1-p2 –> an image of man in not blue colour.\nAlso similar to what we saw in Components of Stable diffusion blog. We used to generate 2 outputs, one aligned with text and one creative. We used to combine by a logic to get the final output.\nIn the Introduction Blog we saw the below code snippet, which generates images from the text.\npipe2 = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16).to(\"cuda\")\n\nprompt = \"a photograph of an astronaut riding a horse\"\n\npipe2(prompt).images[0]\nThe above pipe2 along with prompt also accept a param called negative_prompt.\nLets see some examples and see how good these works.\ntorch.manual_seed(1000)\nprompt = \"man riding blue bike with red dress\"\npipe2(prompt).images[0]\n\nNow lets change the color of bike with negative prompt blue.\ntorch.manual_seed(1000)\npipe2(prompt, negative_prompt=\"blue\").images[0]\n\n\nAs you can see both blue bike and blue pant has been changed.\nLets change the shirt color using negative prompt red.\n\nLet change from riding to something else using negative prompt riding.\n\nNow lets change the vehicle using negative prompt cycle.\n\nNow lets change the vehicle and the colour also using negative prompt blue cycle.\n\nLets give completely opposite of what input prompt given and we will see what the output looks like. negative prompt man riding blue bike.\n\nAs we can see p1 = man riding blue bike with red dress and negative prompt man riding blue bike. Yielded girl in red dress.\nHow this works?\nWe saw in Previous post a method for generating good image using guidance_scale.\nThe equation was :\nfinal_prediction = creative_predection + guidance_scale * predection_based_on_text\nNow lets add negative_prompt to this equation.\nfinal_prediction = creative_predection + guidance_scale * (predection_based_on_text - predection_based_on_negative_text)\nSo all the codes from previous post will be same. Now instead of 2 latents noise , we will doing 3.\n#We will remove noise by X times\nfor i, t in enumerate(tqdm(scheduler.timesteps)):\n    # As we generate for both creative and text , we will have 3 noise inputs\n    input = torch.cat([latents] * 3)\n    input = scheduler.scale_model_input(input, t)\n\n    # Given to unet for prediction\n    with torch.no_grad(): \n        pred = unet(input, t, encoder_hidden_states=text_embeddings).sample\n\n    # perform guidance\n    pred_uncond, pred_text, pred_neg = pred.chunk(3)\n    pred = pred_uncond + guidance_scale * (pred_text - pred_neg)\n\n    # compute the \"previous\" noisy sample\n    latents = scheduler.step(pred, t, latents).prev_sample"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vishwas Pai",
    "section": "",
    "text": "Negative Prompts in Stable Diffusion\n\n\n\n\n\n\n\nCV\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\nVishwas Pai\n\n\n\n\n\n\n  \n\n\n\n\nBuilding components of Stable Diffusion\n\n\n\n\n\n\n\nCV\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2022\n\n\nVishwas Pai\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Stable Diffusion\n\n\n\n\n\n\n\nCV\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2022\n\n\nVishwas Pai\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Socials"
  }
]